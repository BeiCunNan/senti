> creating model bert
> cuda memory allocated: 587888640
> training arguments:
>>> data_dir: data
>>> prompt_lengths: 5
>>> dataset: ie
>>> max_lengths: 75
>>> query_lengths: 16
>>> subject: ie
>>> model_name: bert
>>> method_name: MP-TFWA
>>> train_batch_size: 10
>>> test_batch_size: 64
>>> num_epoch: 2
>>> lr: 1e-05
>>> decay: 0.01
>>> eps: 1e-08
>>> device: cuda
>>> backend: False
>>> workers: 0
>>> timestamp: 1685709672017
>>> num_classes: 3
>>> log_name: bert_MP-TFWA_ie_23-06-02_20-41-12.log
