> creating model bert
> cuda memory allocated: 1477373952
> training arguments:
>>> data_dir: data
>>> prompt_lengths: 5
>>> dataset: ie
>>> max_lengths: 75
>>> query_lengths: 16
>>> subject: idiom
>>> model_name: bert
>>> method_name: MP-TFWA
>>> train_batch_size: 1
>>> test_batch_size: 64
>>> num_epoch: 2
>>> lr: 1e-05
>>> decay: 0.01
>>> eps: 1e-08
>>> device: cuda
>>> backend: False
>>> workers: 0
>>> timestamp: 1687956984686
>>> num_classes: 3
>>> log_name: bert_MP-TFWA_ie_23-06-28_20-56-23.log
